{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "from tensorflow import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random as random\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=40\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Some Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Height = 128  \n",
    "Image_Width = 128\n",
    "Batch_Size = 64\n",
    "ImagePath = './Kideny/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Number Of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Root = Path(ImagePath)\n",
    "\n",
    "Total = 0\n",
    "for sub_dir in Data_Root.iterdir():\n",
    "    Count = len(list(sub_dir.iterdir()))\n",
    "    Total += Count\n",
    "    print(f'{sub_dir.name}: {Count}')\n",
    "\n",
    "print('')\n",
    "print(f'Total Numbers Of Files: {Total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split The Data Into Train, Test, And Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = ImagePath\n",
    "Output = './Data'\n",
    "\n",
    "splitfolders.ratio(Input, output=Output, seed=seed_value, ratio=(.80, 0.1, .1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Image DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Directory = './Data/train'\n",
    "\n",
    "Testing_Directory = './Data/test'\n",
    "\n",
    "Validation_Directory = './Data/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Set = tf.keras.preprocessing.image_dataset_from_directory(directory=Training_Directory,\n",
    "                                                                   image_size=(Image_Height, Image_Width),\n",
    "                                                                   batch_size=Batch_Size,\n",
    "                                                                   label_mode='categorical',\n",
    "                                                                   shuffle=True,\n",
    "                                                                   seed= seed_value,\n",
    "                                                                   color_mode='grayscale',\n",
    "                                                                   labels='inferred',\n",
    "                                                                   class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Set = tf.keras.preprocessing.image_dataset_from_directory(directory=Testing_Directory,\n",
    "                                                                   image_size=(Image_Height, Image_Width),\n",
    "                                                                   batch_size=Batch_Size,\n",
    "                                                                   label_mode='categorical',\n",
    "                                                                   shuffle=True,\n",
    "                                                                   seed= seed_value,\n",
    "                                                                   color_mode='grayscale',\n",
    "                                                                   labels='inferred',\n",
    "                                                                   class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_Set = tf.keras.preprocessing.image_dataset_from_directory(directory=Validation_Directory,\n",
    "                                                                   image_size=(Image_Height, Image_Width),\n",
    "                                                                   batch_size=Batch_Size,\n",
    "                                                                   label_mode='categorical',\n",
    "                                                                   shuffle=True,\n",
    "                                                                   seed= seed_value,\n",
    "                                                                   color_mode='grayscale',\n",
    "                                                                   labels='inferred',\n",
    "                                                                   class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "                                                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_images(Data, NumberOfImages):\n",
    "    \n",
    "    '''Plot Some Images From Training Set, Testing Set And Validation Set With Class Of Selected Image'''\n",
    "\n",
    "    Labels = Data.class_names\n",
    "    Data = Data\n",
    "    NumberOfImages = NumberOfImages\n",
    "\n",
    "    Row = Col  = int(tf.get_static_value(math.ceil(NumberOfImages**0.5)))\n",
    "\n",
    "    Counter = 1\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for images, labels in Data:\n",
    "     for i in range(NumberOfImages):\n",
    "        ax = plt.subplot(Row, Col,Counter)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(Labels[np.argmax(labels[i])], color='green', fontsize = 15)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        if Counter == NumberOfImages:\n",
    "           break\n",
    "        else:\n",
    "           Counter += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Training Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_images(Training_Set, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Testing Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_images(Testing_Set, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_images(Validation_Set, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Shape = (Image_Height, Image_Width, 1)\n",
    "Epochs = 100\n",
    "Number_Of_Classes = len(os.listdir(ImagePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_Training_Set = Training_Set.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Callback = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"accuracy\",\n",
    "                    patience=3,\n",
    "                    verbose=0,\n",
    "                    mode=\"max\",\n",
    "                    restore_best_weights=True,\n",
    "                    start_from_epoch=0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictin_Image1 = r'./Prediction Images/Normal- (30).jpg'\n",
    "Predictin_Image2 = r'./Prediction Images/Cyst- (22).jpg'\n",
    "Predictin_Image3 = r'./Prediction Images/Stone- (5).jpg'\n",
    "Predictin_Image4 = r'./Prediction Images/Tumor- (20).jpg'\n",
    "Predictin_Image5 = r'./Prediction Images/Normal- (76).jpg'\n",
    "Predictin_Image6 = r'./Prediction Images/Stone- (26).jpg'\n",
    "Predictin_Image7 = r'./Prediction Images/Cyst- (38).jpg'\n",
    "Predictin_Image8 = r'./Prediction Images/Tumor- (9).jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(): \n",
    "    def __init__(self,input_shape,epochs) :\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def convolutional_neural_network(self):\n",
    "        '''Build Convolutional Neural Network For Images Classification, Compile It, Anf Fit It'''\n",
    "        model = keras.models.Sequential([\n",
    "                                         Flatten(),\n",
    "                                         Dense(128 ,activation='relu'),\n",
    "                                         Dense(Number_Of_Classes, activation='softmax')])\n",
    "            \n",
    "        model.compile(loss= 'categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        model.fit(Normalized_Training_Set, \n",
    "                  epochs=self.epochs, \n",
    "                  validation_data=Validation_Set,\n",
    "                  callbacks=[Callback],\n",
    "                  steps_per_epoch=None,\n",
    "                  shuffle=True,\n",
    "                  validation_batch_size = None,\n",
    "                  batch_size=None,\n",
    "                  initial_epoch\t = 0)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "\n",
    "    def tranfer_learning(self, base_model):  \n",
    "        '''Define Transfer Learning Models For Images Classification, Compile It, Anf Fit It'''\n",
    "\n",
    "        base_model = base_model\n",
    "        base_model.trainable = False\n",
    "\n",
    "        for layer in base_model.layers[:-4]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        inputs = tf.keras.Input(self.input_shape)\n",
    "        GrayScaleImage = tf.keras.layers.Concatenate()([inputs, inputs, inputs])\n",
    "        x = base_model(GrayScaleImage)\n",
    "        outputs = Dense(Number_Of_Classes, 'softmax')(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "          \n",
    "        model.compile(loss= 'categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        model.fit(Normalized_Training_Set, \n",
    "                  epochs=self.epochs, \n",
    "                  validation_data=Validation_Set,\n",
    "                  callbacks=[Callback],\n",
    "                  steps_per_epoch=len(Training_Set),\n",
    "                  shuffle=True,\n",
    "                  validation_batch_size = len(Validation_Set),\n",
    "                  batch_size=Batch_Size,\n",
    "                  initial_epoch\t= 0)\n",
    "        \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model History Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_history_values(model, name):\n",
    "    '''Export The Model Training Accuracy, Training Loss, Validation Accuracy, Validation Loss To DataFrame And CSV File'''\n",
    "    file_name = name\n",
    "    HistoryResultDF= pd.DataFrame(model.history.history)\n",
    "    HistoryResultDF.to_csv(os.path.join('{}HitoryResult.csv'.format(file_name)) ,index=False)\n",
    "    return HistoryResultDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(ModelHistory): \n",
    "        '''Plot The Model Training Accuracy, Training Loss, Validation Accuracy, Validation Loss '''\n",
    "        figure, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "        ModelHistory[['accuracy', 'val_accuracy']].plot(ax=ax[0],title='Accuracy');\n",
    "        ModelHistory[['loss', 'val_loss']].plot(ax=ax[1], title='Loss');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Model Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelValues():\n",
    "        def __init__(self, HistoryResultDF) :\n",
    "                self.HistoryResultDF = HistoryResultDF\n",
    "\n",
    "        def loss_accuracy_values(self):\n",
    "                '''Find The Mean Value Of The Model Training Accuracy, Training Loss, Validation Accuracy, Validation Loss And Export As Dataframe'''\n",
    "\n",
    "                Loss = self.HistoryResultDF['loss'].mean()\n",
    "                Accuracy = self.HistoryResultDF['accuracy'].mean()\n",
    "                ValLoss = self.HistoryResultDF['val_loss'].mean()\n",
    "                ValAccuracy = self.HistoryResultDF['val_accuracy'].mean()\n",
    "                LossAccuracyValuesDf =  pd.DataFrame(data=[Loss, Accuracy, ValLoss, ValAccuracy], index=['Loss', 'Accuracy', 'Val_Loss', 'Val_Accuracy'], columns=['Values'])\n",
    "\n",
    "                return Loss, Accuracy, ValLoss, ValAccuracy, LossAccuracyValuesDf\n",
    "\n",
    "        def loss_accuracy_valuesـmaximum_values(self):\n",
    "                '''Find The Maximum Value Of The Model Training Accuracy, Training Loss, Validation Accuracy, Validation Loss And Export As Dataframe'''\n",
    "\n",
    "                MaxLoss = self.HistoryResultDF['loss'].max()\n",
    "                MaxAccuracy = self.HistoryResultDF['accuracy'].max()\n",
    "                MaxValLoss = self.HistoryResultDF['val_loss'].max()\n",
    "                MaxValAccuracy = self.HistoryResultDF['val_accuracy'].max()\n",
    "                MaxLossAccuracyValuesDf =  pd.DataFrame(data=[MaxLoss, MaxAccuracy, MaxValLoss, MaxValAccuracy], index=['Max_Loss', 'Max_Accuracy', 'Max_Val_Loss', 'Max_Val_Accuracy'], columns=['Values'])\n",
    "\n",
    "                return MaxLoss, MaxAccuracy, MaxValLoss, MaxValAccuracy, MaxLossAccuracyValuesDf\n",
    "\n",
    "        def loss_accuracy_valuesـminimum_values(self):\n",
    "                '''Find The Minimum Value Of The Model Training Accuracy, Training Loss, Validation Accuracy, Validation Loss And Export As Dataframe'''\n",
    "\n",
    "                MinLoss = self.HistoryResultDF['loss'].min()\n",
    "                MinAccuracy = self.HistoryResultDF['accuracy'].min()\n",
    "                MinValLoss = self.HistoryResultDF['val_loss'].min()\n",
    "                MinValAccuracy = self.HistoryResultDF['val_accuracy'].min()\n",
    "                MinLossAccuracyValuesDf =  pd.DataFrame(data=[MinLoss, MinAccuracy, MinValLoss, MinValAccuracy], index=['Min_Loss', 'Min_Accuracy', 'Min_Val_Loss', 'Min_Val_Accuracy'], columns=['Values'])\n",
    "\n",
    "                return MinLoss, MinAccuracy, MinValLoss, MinValAccuracy, MinLossAccuracyValuesDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evulation(model, data):\n",
    "    '''Find The Testing Accuracy And Testing Loss And Export As Dataframe '''\n",
    "    Loss, Accuracy = model.evaluate(data, verbose=0)\n",
    "\n",
    "    EvaluationValues = pd.DataFrame(index= ['Loss', 'Accuracy'], data = [Loss, Accuracy], columns=['Values'])\n",
    "    \n",
    "    return Loss, Accuracy, EvaluationValues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Real And Prediction Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_prediction_classes(model):\n",
    "    '''Generate The Real And Predection Classes For Use In Confusion Matrix And Performance Metrics '''\n",
    "\n",
    "    Real =  np.array([])\n",
    "    Predictions = np.array([])\n",
    "    \n",
    "    for x, y in Testing_Set:\n",
    "        Real = np.concatenate((Real, np.argmax(y, axis=1)))\n",
    "        y_pred = np.argmax(model.predict(x, verbose=0),axis=1)\n",
    "        Predictions = np.concatenate([Predictions, y_pred])\n",
    "        \n",
    "        if len(Real) == len(Testing_Set.file_paths):\n",
    "            break\n",
    "\n",
    "    return Real, Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_(Real, Prediction):\n",
    "        '''The confusion matrix is used to analyze the ability of model to identify the classes of the dataset'''\n",
    "\n",
    "        ConfusionMatrix = confusion_matrix(Real, Prediction)\n",
    "    \n",
    "        return ConfusionMatrix\n",
    "    \n",
    "def plot_confusion_matrix(ConfusionMatrix, name):\n",
    "    '''Display The Confusion Matrix'''\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=ConfusionMatrix, display_labels = Testing_Set.class_names)\n",
    "    disp.plot(cmap='YlGnBu', colorbar=False, xticks_rotation='vertical', values_format='d')\n",
    "    plt.title('{} Confusion Matrix with Labels'.format(name))\n",
    "    plt.rcParams['font.size'] = '10'\n",
    "    plt.grid(None)\n",
    "    return plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Generate True Positive, False Positive, True Negative, False Negative**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A. Positive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Positive():     \n",
    "    def __init__(self, Confusin_Matrix):\n",
    "        self.Confusin_Matrix = Confusin_Matrix\n",
    "    \n",
    "    def false_positive(self):\n",
    "        '''The prediction is wrong and the real value is positive \n",
    "        Using numpy to find the values of false positive from confusion matrix'''\n",
    "\n",
    "        False_Postive = self.Confusin_Matrix.sum(axis=0) - np.diag(self.Confusin_Matrix)\n",
    "        False_Postive = False_Postive.astype(int)\n",
    "        False_Postive = sum(False_Postive)\n",
    "        \n",
    "        return False_Postive\n",
    "        \n",
    "    def true_positive(self):\n",
    "        ''' The prediction is correct and the real value is positive \n",
    "        Using numpy to find the values of true positive from confusion matrix'''\n",
    "\n",
    "        True_Positive = np.diag(self.Confusin_Matrix)\n",
    "        True_Positive = True_Positive.astype(int)\n",
    "        True_Positive = sum(True_Positive)\n",
    "        \n",
    "        return True_Positive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B. Negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Negative():\n",
    "    \n",
    "    def __init__(self, Confusin_Matrix):\n",
    "        self.Confusin_Matrix = Confusin_Matrix\n",
    "    \n",
    "    def false_negative(self):\n",
    "        '''The prediction is wrong and the real value is negative \n",
    "        Using numpy to find the values of false negative from confusion matrix'''\n",
    "\n",
    "        False_Negative = self.Confusin_Matrix.sum(axis=1) - np.diag(self.Confusin_Matrix)\n",
    "        False_Negative = False_Negative.astype(int)\n",
    "        False_Negative = sum(False_Negative)\n",
    "        \n",
    "        return False_Negative \n",
    "    \n",
    "    def true_negative(self):\n",
    "         ''' The prediction is correct and the real value is negative \n",
    "         Using numpy to find the values of true negative from confusion matrix'''\n",
    "\n",
    "         True_Negative = self.Confusin_Matrix.sum()\n",
    "         X = (self.Confusin_Matrix.sum(axis=0) - np.diag(self.Confusin_Matrix)) + (self.Confusin_Matrix.sum(axis=1) - np.diag(self.Confusin_Matrix)) + np.diag(self.Confusin_Matrix)\n",
    "         True_Negative = True_Negative.astype(int) - X\n",
    "         True_Negative = sum(True_Negative)\n",
    "         \n",
    "         return True_Negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Performance Merics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Performance_Metrics:\n",
    "    \n",
    "    def __init__(self, True_Positive, True_Negative, False_Positive, False_Negative):\n",
    "        self.True_Positive = True_Positive\n",
    "        self.True_Negative = True_Negative\n",
    "        self.False_Positive = False_Positive\n",
    "        self.False_Negative = False_Negative\n",
    "        \n",
    "            \n",
    "    def accuracy_score(self):\n",
    "        ''' Accuracy is using to measure the accurate of the model '''\n",
    "        Accuracy = (self.True_Positive + self.True_Negative) / (self.True_Positive + self.True_Negative + self.False_Positive + self.False_Negative)\n",
    "\n",
    "        return Accuracy\n",
    "    \n",
    "    def precision_score(self):\n",
    "        '''Precision is used to identified is the predicted result is close to the real result'''\n",
    "        \n",
    "        Precision = ((self.True_Positive) / (self.True_Positive + self.False_Positive))\n",
    "        \n",
    "        return Precision\n",
    "    \n",
    "    def sensitivity_score(self):\n",
    "        '''Sensitivity is true positive rate measure'''\n",
    "\n",
    "        Sensitivity = ((self.True_Positive ) / (self.True_Positive  + self.False_Negative)) \n",
    "\n",
    "        return Sensitivity\n",
    "     \n",
    "    def specificity_score(self):\n",
    "        '''Specificity is true negative rate measure'''\n",
    "\n",
    "        Specificity = ((self.True_Negative) / (self.True_Negative + self.False_Negative)) \n",
    "\n",
    "        return Specificity\n",
    "    \n",
    "    def f1_score(self):\n",
    "        '''F-Measure refers to the mean of consistency between precision and sensitivity'''\n",
    "        \n",
    "        F1Score = (((self.True_Positive ) / (self.True_Positive  + self.False_Negative) * (self.True_Positive) / (self.True_Positive + self.False_Positive)) / ((self.True_Positive ) / (self.True_Positive  + self.False_Negative) + (self.True_Positive) / (self.True_Positive + self.False_Positive))) * 2\n",
    "\n",
    "        return F1Score\n",
    "    \n",
    "    def err_rate(self):\n",
    "        Error = ((self.False_Positive + self.False_Negative) / (self.True_Positive + self.True_Negative + self.False_Positive + self.False_Negative)) \n",
    "  \n",
    "        return Error \n",
    "    \n",
    "    @staticmethod \n",
    "    def display_performance_metrics_values(Accuracy, Precision, Sensitivity, Specificity, F1Score, Error):\n",
    "        MetricsValues = pd.DataFrame(index=['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1Score', 'Error'], columns=['Values'], data=[Accuracy, Precision, Sensitivity, Specificity, F1Score, Error])\n",
    "\n",
    "        return MetricsValues   \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prediction):\n",
    "    if np.argmax(prediction) == 0:\n",
    "        print('Cyst')\n",
    "    elif np.argmax(prediction) == 1:\n",
    "        print('Normal')   \n",
    "    elif np.argmax(prediction) == 2:\n",
    "        print('Stone')    \n",
    "    elif np.argmax(prediction) == 3:\n",
    "        print('Tumor')\n",
    "    else:\n",
    "        print('Unkown')\n",
    "\n",
    "def predict_image(model, path):\n",
    "\n",
    "    PredictionImage = tf.keras.utils.load_img(path, target_size=(Image_Height, Image_Width, 1))\n",
    "    PredictionImageArray = tf.keras.utils.img_to_array(PredictionImage)\n",
    "    PredictionImageArray = np.array([PredictionImageArray])\n",
    "    GrayScaleImages = tf.image.rgb_to_grayscale(PredictionImageArray)\n",
    "    predictions = model.predict(GrayScaleImages)\n",
    "    Predction = predict(predictions)\n",
    "    return Predction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Convolutional Neural Network'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "    model = Models(Input_Shape, Epochs)\n",
    "    model0 = model.convolutional_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF0 = model_history_values(model0, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues0 = ModelValues(HistoryResultDF0)\n",
    "Loss0, Accuracy0, ValLoss0, ValAccuracy0, LossAccuracyValuesDf0 = ModelValues0.loss_accuracy_values()\n",
    "MaxLoss0, MaxAccuracy0, MaxValLoss0, MaxValAccuracy0, MaxLossAccuracyValuesDf0 = ModelValues0.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss0, MinAccuracy0, MinValLoss0, MinValAccuracy0, MinLossAccuracyValuesDf0 = ModelValues0.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss0, TestingAccuracy0, EvaluationValues0 = model_evulation(model0, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real0, Predictions0 = generate_real_prediction_classes(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix0 = confusion_matrix_(Real0, Predictions0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ConfusionMatrix0, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive0 = Generate_Positive(ConfusionMatrix0)\n",
    "TruePositive0 = Positive0.true_positive()\n",
    "FalsePostive0 = Positive0.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative0 = Generate_Negative(ConfusionMatrix0)\n",
    "TrueNegative0 = Negative0.true_negative()\n",
    "FalseNegative0 = Negative0.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance0 = Performance_Metrics(TruePositive0, TrueNegative0 ,FalsePostive0, FalseNegative0)\n",
    "\n",
    "Accuracy0 = Performance0.accuracy_score()\n",
    "Precision0 = Performance0.precision_score()\n",
    "Sensitivity0 = Performance0.sensitivity_score()\n",
    "Specificity0 = Performance0.specificity_score()\n",
    "F1Score0 = Performance0.f1_score()\n",
    "ErrorRate0 = Performance0.err_rate()\n",
    "\n",
    "PerformanceValues0 = Performance0.display_performance_metrics_values(Accuracy0, Precision0, Sensitivity0, Specificity0, F1Score0, ErrorRate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model0, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. InceptionV3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'InceptionV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "    base_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    weights='imagenet',\n",
    "                    pooling = 'avg',\n",
    "                    )\n",
    "    \n",
    "    Inc = Models(Input_Shape, Epochs)\n",
    "    model1 = Inc.tranfer_learning(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF1 = model_history_values(model1, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues1 = ModelValues(HistoryResultDF1)\n",
    "Loss1, Accuracy1, ValLoss1, ValAccuracy1, LossAccuracyValuesDf1 = ModelValues1.loss_accuracy_values()\n",
    "MaxLoss1, MaxAccuracy1, MaxValLoss1, MaxValAccuracy1, MaxLossAccuracyValuesDf1 = ModelValues1.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss1, MinAccuracy1, MinValLoss1, MinValAccuracy1, MinLossAccuracyValuesDf1 = ModelValues1.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss1, TestingAccuracy1, EvaluationValues1 = model_evulation(model1, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real1, Predictions1 = generate_real_prediction_classes(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix1 = confusion_matrix_(Real1, Predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive1 = Generate_Positive(ConfusionMatrix1)\n",
    "TruePositive1 = Positive1.true_positive()\n",
    "FalsePostive1 = Positive1.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative1 = Generate_Negative(ConfusionMatrix1)\n",
    "TrueNegative1 = Negative1.true_negative()\n",
    "FalseNegative1 = Negative1.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance1 = Performance_Metrics(TruePositive1, TrueNegative1 ,FalsePostive1, FalseNegative1)\n",
    "\n",
    "Accuracy1 = Performance1.accuracy_score()\n",
    "Precision1 = Performance1.precision_score()\n",
    "Sensitivity1 = Performance1.sensitivity_score()\n",
    "Specificity1 = Performance1.specificity_score()\n",
    "F1Score1 = Performance1.f1_score()\n",
    "ErrorRate1 = Performance1.err_rate()\n",
    "\n",
    "PerformanceValues1 = Performance1.display_performance_metrics_values(Accuracy1, Precision1, Sensitivity1, Specificity1, F1Score1, ErrorRate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model1, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. DenseNet121**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'DenseNet121'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "        base_model = tf.keras.applications.densenet.DenseNet121(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                 pooling = 'avg',                    \n",
    "                )\n",
    "\n",
    "        NetV2B0 = Models(Input_Shape, Epochs)\n",
    "        model2 = NetV2B0.tranfer_learning(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF2 = model_history_values(model2, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues2 = ModelValues(HistoryResultDF2)\n",
    "Loss2, Accuracy2, ValLoss2, ValAccuracy2, LossAccuracyValuesDf2 = ModelValues2.loss_accuracy_values()\n",
    "MaxLoss2, MaxAccuracy2, MaxValLoss2, MaxValAccuracy2, MaxLossAccuracyValuesDf2 = ModelValues2.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss2, MinAccuracy2, MinValLoss2, MinValAccuracy2, MinLossAccuracyValuesDf2 = ModelValues2.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss2, TestingAccuracy2, EvaluationValues2 = model_evulation(model2, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real2, Predictions2 = generate_real_prediction_classes(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix2 = confusion_matrix_(Real2, Predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ConfusionMatrix2, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive2 = Generate_Positive(ConfusionMatrix2)\n",
    "TruePositive2= Positive2.true_positive()\n",
    "FalsePostive2 = Positive2.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative2 = Generate_Negative(ConfusionMatrix2)\n",
    "TrueNegative2 = Negative2.true_negative()\n",
    "FalseNegative2 = Negative2.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance2 = Performance_Metrics(TruePositive2, TrueNegative2 ,FalsePostive2, FalseNegative2)\n",
    "\n",
    "Accuracy2 = Performance2.accuracy_score()\n",
    "Precision2 = Performance2.precision_score()\n",
    "Sensitivity2 = Performance2.sensitivity_score()\n",
    "Specificity2 = Performance2.specificity_score()\n",
    "F1Score2 = Performance2.f1_score()\n",
    "ErrorRate2 = Performance2.err_rate()\n",
    "\n",
    "PerformanceValues2 = Performance2.display_performance_metrics_values(Accuracy2, Precision2, Sensitivity2, Specificity2, F1Score2, ErrorRate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model2, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. MobileNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MobileNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "        base_model = tf.keras.applications.mobilenet.MobileNet(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                pooling = 'avg',\n",
    "                )\n",
    "\n",
    "        Mobile = Models(Input_Shape, Epochs)\n",
    "        model3 = Mobile.tranfer_learning(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF3 = model_history_values(model3, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues3 = ModelValues(HistoryResultDF3)\n",
    "Loss3, Accuracy3, ValLoss3, ValAccuracy3, LossAccuracyValuesDf3 = ModelValues3.loss_accuracy_values()\n",
    "MaxLoss3, MaxAccuracy3, MaxValLoss3, MaxValAccuracy3, MaxLossAccuracyValuesDf3 = ModelValues3.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss3, MinAccuracy3, MinValLoss3, MinValAccuracy3, MinLossAccuracyValuesDf3 = ModelValues3.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss3, TestingAccuracy3, EvaluationValues3 = model_evulation(model3, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real3, Predictions3 = generate_real_prediction_classes(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix3 = confusion_matrix_(Real3, Predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ConfusionMatrix3, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive3 = Generate_Positive(ConfusionMatrix3)\n",
    "TruePositive3 = Positive3.true_positive()\n",
    "FalsePostive3 = Positive3.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative3 = Generate_Negative(ConfusionMatrix3)\n",
    "TrueNegative3 = Negative3.true_negative()\n",
    "FalseNegative3 = Negative3.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance3 = Performance_Metrics(TruePositive3, TrueNegative3 ,FalsePostive3, FalseNegative3)\n",
    "\n",
    "Accuracy3 = Performance3.accuracy_score()\n",
    "Precision3 = Performance3.precision_score()\n",
    "Sensitivity3 = Performance3.sensitivity_score()\n",
    "Specificity3 = Performance3.specificity_score()\n",
    "F1Score3 = Performance3.f1_score()\n",
    "ErrorRate3 = Performance3.err_rate()\n",
    "\n",
    "PerformanceValues3 = Performance3.display_performance_metrics_values(Accuracy3, Precision3, Sensitivity3, Specificity3, F1Score3, ErrorRate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model3, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Xception'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "        base_model = tf.keras.applications.xception.Xception(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                pooling = 'avg',\n",
    "        )\n",
    "\n",
    "        XCE = Models(Input_Shape, Epochs)\n",
    "        model4 = XCE.tranfer_learning(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF4= model_history_values(model4, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues4 = ModelValues(HistoryResultDF4)\n",
    "Loss4, Accuracy4, ValLoss4, ValAccuracy4, LossAccuracyValuesDf4 = ModelValues4.loss_accuracy_values()\n",
    "MaxLoss4, MaxAccuracy4, MaxValLoss4, MaxValAccuracy4, MaxLossAccuracyValuesDf4 = ModelValues4.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss4, MinAccuracy4, MinValLoss4, MinValAccuracy4, MinLossAccuracyValuesDf4 = ModelValues4.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss4, TestingAccuracy4, EvaluationValues4 = model_evulation(model4, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real4, Predictions4 = generate_real_prediction_classes(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix4 = confusion_matrix_(Real4, Predictions4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive4 = Generate_Positive(ConfusionMatrix4)\n",
    "TruePositive4 = Positive4.true_positive()\n",
    "FalsePostive4 = Positive4.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative4 = Generate_Negative(ConfusionMatrix4)\n",
    "TrueNegative4 = Negative1.true_negative()\n",
    "FalseNegative4 = Negative1.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance4 = Performance_Metrics(TruePositive4, TrueNegative4 ,FalsePostive4, FalseNegative4)\n",
    "\n",
    "Accuracy4 = Performance4.accuracy_score()\n",
    "Precision4 = Performance4.precision_score()\n",
    "Sensitivity4 = Performance4.sensitivity_score()\n",
    "Specificity4 = Performance4.specificity_score()\n",
    "F1Score4 = Performance4.f1_score()\n",
    "ErrorRate4 = Performance4.err_rate()\n",
    "\n",
    "PerformanceValues4 = Performance4.display_performance_metrics_values(Accuracy4, Precision4, Sensitivity4, Specificity4, F1Score4, ErrorRate4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model4, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ResNet50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('GPU'):\n",
    "        base_model = tf.keras.applications.resnet50.ResNet50( \n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                pooling = 'avg',\n",
    "                )\n",
    "        RES = Models(Input_Shape, Epochs)\n",
    "        model5 = RES.tranfer_learning(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF5= model_history_values(model5, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistoryResultDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(HistoryResultDF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelValues5 = ModelValues(HistoryResultDF5)\n",
    "Loss5, Accuracy5, ValLoss5, ValAccuracy5, LossAccuracyValuesDf5 = ModelValues5.loss_accuracy_values()\n",
    "MaxLoss5, MaxAccuracy5, MaxValLoss5, MaxValAccuracy5, MaxLossAccuracyValuesDf5 = ModelValues5.loss_accuracy_valuesـmaximum_values()\n",
    "MinLoss5, MinAccuracy5, MinValLoss5, MinValAccuracy5, MinLossAccuracyValuesDf5 = ModelValues5.loss_accuracy_valuesـminimum_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossAccuracyValuesDf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxLossAccuracyValuesDf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinLossAccuracyValuesDf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingLoss5, TestingAccuracy5, EvaluationValues5 = model_evulation(model5, Testing_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationValues5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real5, Predictions5 = generate_real_prediction_classes(model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix5 = confusion_matrix_(Real5, Predictions5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive5 = Generate_Positive(ConfusionMatrix5)\n",
    "TruePositive5 = Positive5.true_positive()\n",
    "FalsePostive5 = Positive5.false_positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative5 = Generate_Negative(ConfusionMatrix5)\n",
    "TrueNegative5 = Negative5.true_negative()\n",
    "FalseNegative5 = Negative5.false_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance5 = Performance_Metrics(TruePositive5, TrueNegative5 ,FalsePostive5, FalseNegative5)\n",
    "\n",
    "Accuracy5 = Performance5.accuracy_score()\n",
    "Precision5 = Performance5.precision_score()\n",
    "Sensitivity5 = Performance5.sensitivity_score()\n",
    "Specificity5 = Performance5.specificity_score()\n",
    "F1Score5 = Performance5.f1_score()\n",
    "ErrorRate5 = Performance5.err_rate()\n",
    "\n",
    "PerformanceValues5 = Performance5.display_performance_metrics_values(Accuracy5, Precision5, Sensitivity5, Specificity5, F1Score5, ErrorRate5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceValues5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(model5, Predictin_Image8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = {\n",
    "        'Training_Loss':[Loss0, Loss1, Loss2, Loss3, Loss4, Loss5],\n",
    "         'Training_Accuracy': [Accuracy0, Accuracy1, Accuracy2, Accuracy3, Accuracy4, Accuracy5],\n",
    "         'Val_Loss': [ValLoss0, ValLoss1, ValLoss2, ValLoss3, ValLoss4, ValLoss5],\n",
    "         'Val_Accuracy': [ValAccuracy0, ValAccuracy1, ValAccuracy2, ValAccuracy3, ValAccuracy4, ValAccuracy5],\n",
    "         'Testing_Loss':[TestingLoss0, TestingLoss1, TestingLoss2, TestingLoss3, TestingLoss4, TestingLoss5],\n",
    "         'Testing_Accuracy':[TestingAccuracy0, TestingAccuracy1, TestingAccuracy2, TestingAccuracy3, TestingAccuracy4, TestingAccuracy5],\n",
    "         'Accuracy':[Accuracy0, Accuracy1, Accuracy2, Accuracy3, Accuracy4, Accuracy5],\n",
    "         'Precision':[Precision0, Precision1, Precision2, Precision3, Precision4, Precision5],\n",
    "         'Sensitivity':[Sensitivity0, Sensitivity1, Sensitivity2, Sensitivity3, Sensitivity4, Sensitivity5],\n",
    "         'Specificity':[Specificity0, Specificity1, Specificity2, Specificity3, Specificity4, Specificity5],\n",
    "         'F1Score': [F1Score0, F1Score1, F1Score2, F1Score3, F1Score4, F1Score5],\n",
    "         'ErrorRate':[ErrorRate0, ErrorRate1, ErrorRate2, ErrorRate3, ErrorRate4, ErrorRate5,]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult = pd.DataFrame(Result, index = ['Convolutional Neural Network', 'InceptionV3', 'DenseNet121', 'MobileNet', 'Xception', 'ResNet50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllResult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
